{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccfc985a-1e2b-4bce-a97c-944198464284",
   "metadata": {},
   "source": [
    "# Psitron Technologies Pvt.Ltd [Model monitoring Example]\n",
    "## Build, train, deploy, and monitor a machine learning model with Amazon SageMaker Studio\n",
    "\n",
    "#### In this tutorial, you learn how to:\n",
    "\n",
    "1. Set up the Amazon SageMaker Studio Control Panel\n",
    "1. Download a public dataset using an Amazon SageMaker Studio Notebook and upload it to Amazon S3\n",
    "1. Create an Amazon SageMaker Experiment to track and manage training and processing jobs\n",
    "1. Run an Amazon SageMaker Processing job to generate features from raw data\n",
    "1. Train a model using the built-in XGBoost algorithm\n",
    "1. Test the model performance on the test dataset using Amazon SageMaker Batch Transform\n",
    "1. Deploy the model as an endpoint, and set up a Monitoring job to monitor the model endpoint in production for data drift.\n",
    "1. Visualize results and monitor the model using SageMaker Model Monitor to determine any differences between the training dataset and the deployed model.\n",
    "\n",
    "#### Step 1. Create an AWS Account\n",
    "#### Step 2. Create your Amazon SageMaker Studio Control Panel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7899d6-cc95-43b9-9ecf-6e214960aff4",
   "metadata": {},
   "source": [
    "# Step 3. Download the dataset\n",
    "\n",
    "#### a.In JupyterLab, on the File menu, choose New, then Notebook. In the Select Kernel box, choose Python 3 (Data Science).\n",
    "\n",
    "#### b.First, verify your version of the Amazon SageMaker Python SDK. Copy and paste the following code block into the code cell and select Run.\n",
    " \n",
    "#### Note: While the code runs, an * appears between the square brackets. After a few seconds, the code execution completes and the * is replaced with a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e522e-253d-4e4a-ac12-f770fef7b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import sys\n",
    "import IPython\n",
    "\n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    print(\"Installing previous SageMaker Version and restarting the kernel\")\n",
    "    !{sys.executable} -m pip install sagemaker==1.72.0\n",
    "    IPython.Application.instance().kernel.do_shutdown(True)\n",
    "\n",
    "else:\n",
    "    print(\"Version is good\")\n",
    "\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.session.Session().region_name\n",
    "print(\"Region = {}\".format(region))\n",
    "sm = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abdf151-c2e4-4402-b155-193d4996b453",
   "metadata": {},
   "source": [
    "#### Next, import libraries. Copy and paste the following code into the code cell and select Run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b531474-36fc-43ec-aad3-6c56e89368f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep, gmtime, strftime\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b632c1-624b-4c0d-a45d-a4ebd769b1d9",
   "metadata": {},
   "source": [
    "#### c.Finally, import the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12a794-54f5-4983-9af9-2d29723c9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sagemaker-experiments \n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fda603-032f-46e8-9fc9-b46fa5d11b40",
   "metadata": {},
   "source": [
    "####  Define the Amazon S3 buckets and folders for the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5b2b3-0b69-4693-aae4-0a82cf46429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rawbucket= sess.default_bucket() # Alternatively you can use our custom bucket here. \n",
    "\n",
    "prefix = 'sagemaker-modelmonitor' # use this prefix to store all files pertaining to this workshop.\n",
    "\n",
    "dataprefix = prefix + '/data'\n",
    "traindataprefix = prefix + '/train_data'\n",
    "testdataprefix = prefix + '/test_data'\n",
    "testdatanolabelprefix = prefix + '/test_data_no_label'\n",
    "trainheaderprefix = prefix + '/train_headers'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4a3358-d57a-4ee8-a0de-7a5a51bd6399",
   "metadata": {},
   "source": [
    "#### d.Download the dataset and import it using the pandas library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3874d1c-79cc-4826-9fbf-536e39468391",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\n",
    "data = pd.read_excel('default of credit card clients.xls', header=1)\n",
    "data = data.drop(columns = ['ID'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14bcbe-23a3-4be1-b9a1-c7e21a013035",
   "metadata": {},
   "source": [
    "#### e.Rename the last column as Label and extract the label column separately. For the Amazon SageMaker built-in XGBoost algorithm, the label column must be the first column in the dataframe. To make that change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ea9665-1eef-4e41-9255-333e87a0ea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={\"default payment next month\": \"Label\"}, inplace=True)\n",
    "lbl = data.Label\n",
    "data = pd.concat([lbl, data.drop(columns=['Label'])], axis = 1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2f21c-5c26-451f-b008-39fd5c5d0390",
   "metadata": {},
   "source": [
    "#### f.Upload the CSV dataset into an Amazon S3 bucket. Copy and paste the following code into a new code cell and choose Run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45bd58-4727-41f2-97cf-b6e666eb72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('rawdata/rawdata.csv'):\n",
    "    !mkdir rawdata\n",
    "    data.to_csv('rawdata/rawdata.csv', index=None)\n",
    "else:\n",
    "    pass\n",
    "# Upload the raw dataset\n",
    "raw_data_location = sess.upload_data('rawdata', bucket=rawbucket, key_prefix=dataprefix)\n",
    "print(raw_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b623a753-79bd-4e81-9f0f-97820b59033c",
   "metadata": {},
   "source": [
    "# Step 4: Process the data using Amazon SageMaker Processing\n",
    "\n",
    "##### In this step, you use Amazon SageMaker Processing to pre-process the dataset, including scaling the columns and splitting the dataset into train and test data. Amazon SageMaker Processing lets you run your preprocessing, postprocessing, and model evaluation workloads on fully managed infrastructure.\n",
    "\n",
    "##### Complete the following steps to processs the data and generate features using Amazon SageMaker Processing.\n",
    "##### Note: Amazon SageMaker Processing runs on separate compute instances from your notebook. This means you can continue to experiment and run code in your notebook while the processing job is under way. This will incur additional charges for the cost of the instance which is up and running for the duration of the processing job. The instances are automatically terminated by SageMaker once the processing job completes. For pricing details, see Amazon SageMaker Pricing.\n",
    "\n",
    "#### a.Import the scikit-learn processing container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f837c4f-e5a6-4fd8-94ce-c16f4b584536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.c4.xlarge',\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e377dbaa-8a07-4f6d-ae02-9017e569c9a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### b.Run this pre-processing script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4945fd-1613-4cde-8768-e77f9d8bb0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.3)\n",
    "    parser.add_argument('--random-split', type=int, default=0)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    print('Received arguments {}'.format(args))\n",
    "\n",
    "    input_data_path = os.path.join('/opt/ml/processing/input', 'rawdata.csv')\n",
    "    \n",
    "    print('Reading input data from {}'.format(input_data_path))\n",
    "    df = pd.read_csv(input_data_path)\n",
    "    df.sample(frac=1)\n",
    "    \n",
    "    COLS = df.columns\n",
    "    newcolorder = ['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n",
    "    \n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    random_state=args.random_split\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop('Label', axis=1), df['Label'], \n",
    "                                                        test_size=split_ratio, random_state=random_state)\n",
    "    \n",
    "    preprocess = make_column_transformer(\n",
    "        (['PAY_AMT1'], StandardScaler()),\n",
    "        (['BILL_AMT1'], MinMaxScaler()),\n",
    "    remainder='passthrough')\n",
    "    \n",
    "    print('Running preprocessing and feature engineering transformations')\n",
    "    train_features = pd.DataFrame(preprocess.fit_transform(X_train), columns = newcolorder)\n",
    "    test_features = pd.DataFrame(preprocess.transform(X_test), columns = newcolorder)\n",
    "    \n",
    "    # concat to ensure Label column is the first column in dataframe\n",
    "    train_full = pd.concat([pd.DataFrame(y_train.values, columns=['Label']), train_features], axis=1)\n",
    "    test_full = pd.concat([pd.DataFrame(y_test.values, columns=['Label']), test_features], axis=1)\n",
    "    \n",
    "    print('Train data shape after preprocessing: {}'.format(train_features.shape))\n",
    "    print('Test data shape after preprocessing: {}'.format(test_features.shape))\n",
    "    \n",
    "    train_features_headers_output_path = os.path.join('/opt/ml/processing/train_headers', 'train_data_with_headers.csv')\n",
    "    \n",
    "    train_features_output_path = os.path.join('/opt/ml/processing/train', 'train_data.csv')\n",
    "    \n",
    "    test_features_output_path = os.path.join('/opt/ml/processing/test', 'test_data.csv')\n",
    "    \n",
    "    print('Saving training features to {}'.format(train_features_output_path))\n",
    "    train_full.to_csv(train_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")\n",
    "    \n",
    "    print(\"Save training data with headers to {}\".format(train_features_headers_output_path))\n",
    "    train_full.to_csv(train_features_headers_output_path, index=False)\n",
    "                 \n",
    "    print('Saving test features to {}'.format(test_features_output_path))\n",
    "    test_full.to_csv(test_features_output_path, header=False, index=False)\n",
    "    print(\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e9efc-fc43-4f5f-91e4-446e73d76b1b",
   "metadata": {},
   "source": [
    "#### c. Copy the preprocessing code over to the Amazon S3 bucket using the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8fb07-31f4-4483-862c-036016a137fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the preprocessing code over to the s3 bucket\n",
    "codeprefix = prefix + '/code'\n",
    "codeupload = sess.upload_data('preprocessing.py', bucket=rawbucket, key_prefix=codeprefix)\n",
    "print(codeupload)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d3e0b-e096-4785-88ad-d51da8518794",
   "metadata": {},
   "source": [
    "#### d. Specify where you want to store your training and test data after the SageMaker Processing job completes. Amazon SageMaker Processing automatically stores the data in the specified location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5b3e0d-3c17-4eb9-8e68-d3d50437b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_location = rawbucket + '/' + traindataprefix\n",
    "test_data_location = rawbucket+'/'+testdataprefix\n",
    "print(\"Training data location = {}\".format(train_data_location))\n",
    "print(\"Test data location = {}\".format(test_data_location))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3533366e-280c-4c37-a9e5-3c21fbaff1a5",
   "metadata": {},
   "source": [
    "#### e. Copy and paste the following code to start the Processing job. This code starts the job by calling sklearn_processor.run and extracts some optional metadata about the processing job, such as where the training and test outputs were stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1500aabd-bd74-42ab-8d43-dc081765d59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor.run(code=codeupload,\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=raw_data_location,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train',\n",
    "                               destination='s3://' + train_data_location),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test',\n",
    "                                               destination=\"s3://\"+test_data_location),\n",
    "                               ProcessingOutput(output_name='train_data_headers',\n",
    "                                                source='/opt/ml/processing/train_headers',\n",
    "                                               destination=\"s3://\" + rawbucket + '/' + prefix + '/train_headers')],\n",
    "                      arguments=['--train-test-split-ratio', '0.2']\n",
    "                     )\n",
    "\n",
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_training_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'test_data':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77557b4a-9e6d-45d4-b3b1-5116797aee86",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 5: Create an Amazon SageMaker Experiment\n",
    "\n",
    "##### Now that you have downloaded and staged your dataset in Amazon S3, you can create an Amazon SageMaker Experiment. An experiment is a collection of processing and training jobs related to the same machine learning project. Amazon SageMaker Experiments automatically manages and tracks your training runs for you.\n",
    "##### Complete the following steps to create a new experiment.\n",
    "\n",
    "#### Run this code to create an experiment named Build-train-deploy-."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cbae6b-9efc-4b03-991b-b5e3a43dfc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker Experiment\n",
    "cc_experiment = Experiment.create(\n",
    "    experiment_name=f\"Build-train-deploy-{int(time.time())}\", \n",
    "    description=\"Predict credit card default from payments data\", \n",
    "    sagemaker_boto_client=sm)\n",
    "print(cc_experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a5019a-b8f1-48cf-9e39-7346d14a142b",
   "metadata": {},
   "source": [
    "##### Every training job is logged as a trial. Each trial is an iteration of your end-to-end training job. In addition to the training job, it can also track pre-processing and post-processing jobs as well as datasets and other metadata. A single experiment can include multiple trials which makes it easy for you to track multiple iterations over time within the Amazon SageMaker Studio Experiments pane."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534273fa-2719-4468-94b7-f311268e198d",
   "metadata": {},
   "source": [
    "#### b. Copy and paste the following code to track your pre-processing job under Experiments as well as a step in the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edad11f-107b-47ff-bfba-dd4d7c89d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Tracking parameters used in the Pre-processing pipeline.\n",
    "with Tracker.create(display_name=\"Preprocessing\", sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        \"train_test_split_ratio\": 0.2,\n",
    "        \"random_state\":0\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "    tracker.log_input(name=\"ccdefault-raw-dataset\", media_type=\"s3/uri\", value=raw_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-train-dataset\", media_type=\"s3/uri\", value=train_data_location)\n",
    "    tracker.log_input(name=\"ccdefault-test-dataset\", media_type=\"s3/uri\", value=test_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aaac87-dd3f-4525-8330-01b14259874c",
   "metadata": {},
   "source": [
    "#### c. View the details of the experiment: In the Experiments pane, right-click the experiment named Build-train-deploy- and choose Open in trial components list.\n",
    "\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-trial-components.a29bc386558de92b10e5681c9f17ab6f58578495.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff5f736-6246-4a29-a653-303f5392d394",
   "metadata": {},
   "source": [
    "#### d. Copy and paste the following code and choose Run. Then, take a closer look at the code:\n",
    "\n",
    "##### To train an XGBoost classifier, you first import the XGBoost container maintained by Amazon SageMaker. Then, you log the training run under a Trial so SageMaker Experiments can track it under a Trial name. The pre-processing job is included under the same trial name since it is part of the pipeline. Next, create a SageMaker Estimator object, which automatically provisions the underlying instance type of your choosing, copies over the training data from the specified output location from the processing job, trains the model, and outputs the model artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510f88f0-3c6c-4b36-b274-07d1d4d1eb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost', '1.0-1')\n",
    "s3_input_train = sagemaker.s3_input(s3_data='s3://' + train_data_location, content_type='csv')\n",
    "preprocessing_trial_component = tracker.trial_component\n",
    "\n",
    "trial_name = f\"cc-default-training-job-{int(time.time())}\"\n",
    "cc_trial = Trial.create(\n",
    "        trial_name=trial_name, \n",
    "            experiment_name=cc_experiment.experiment_name,\n",
    "        sagemaker_boto_client=sm\n",
    "    )\n",
    "\n",
    "cc_trial.add_trial_component(preprocessing_trial_component)\n",
    "cc_training_job_name = \"cc-training-job-{}\".format(int(time.time()))\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    train_max_run=86400,\n",
    "                                    output_path='s3://{}/{}/models'.format(rawbucket, prefix),\n",
    "                                    sagemaker_session=sess) # set to true for distributed training\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        verbosity=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit(inputs = {'train':s3_input_train},\n",
    "       job_name=cc_training_job_name,\n",
    "        experiment_config={\n",
    "            \"TrialName\": cc_trial.trial_name, #log training job in Trials for lineage\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "        wait=True,\n",
    "    )\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e945b-58b7-4e59-9fe0-d3c1ab89d30d",
   "metadata": {},
   "source": [
    "#### e. In the left toolbar, choose Experiment. Right-click the Build-train-deploy- experiment and choose Open in trial components list. Amazon SageMaker Experiments captures all the runs including any failed training runs.\n",
    "\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-job-list.f741f928cbf3c41cfb829696e624a30cbad11d9f.png\" alt=\"Italian Trulli\">\n",
    "\n",
    "#### f. Right-click one of the completed Training jobs and choose Open in Trial Details to explore the associated metadata with the training job.  \n",
    "\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-training-metadata.57bcdc852f9f9f450dbb3a1d2a675ee9a27dae90.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d0e435-8067-44f9-b808-5c9d8530667e",
   "metadata": {},
   "source": [
    "# Step 6: Deploy the model for offline inference\n",
    "\n",
    "##### In your preprocessing step, you generated some test data. In this step, you generate offline or batch inference from the trained model to evaluate the model performance on unseen test data.\n",
    "##### Complete the following steps to deploy the model for offline inference.\n",
    "\n",
    "#### a.This step copies the test dataset over from the Amazon S3 location into your local folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4899f272-d5ec-42b5-a12a-1ff5ceebed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = 's3://' + test_data_location + '/test_data.csv'\n",
    "! aws s3 cp $test_data_path ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be08e53-dffa-4715-9b1d-14e0e523e5f0",
   "metadata": {},
   "source": [
    "#### b. Copy and paste the following code and choose Run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645462c-e2fc-4880-a2aa-d4325500e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_full = pd.read_csv('test_data.csv', names = [str(x) for x in range(len(data.columns))])\n",
    "test_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410a57c7-e03e-410c-9938-79c55b5a739c",
   "metadata": {},
   "source": [
    "#### c. Copy and paste the following code and choose Run. This step extracts the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab66d6-40ad-4185-a8da-b16626aca12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label = test_full['0'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65aa69-b7e7-4a7f-9342-f689af8aaade",
   "metadata": {},
   "source": [
    "### d. Copy and paste the following code and choose Run to create the Batch Transform job. Then, take a closer look at the code:\n",
    "\n",
    "##### Like the training job, SageMaker provisions all the underlying resources, copies over the trained model artifacts, sets up a Batch endpoint locally, copies over the data, and runs inferences on the data and pushes the outputs to Amazon S3. Note that by setting the input_filter, you are letting Batch Transform know to neglect the first column in the test data which is the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9a6273-c4e2-44d7-9082-63cb7cf0de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sm_transformer = xgb.transformer(1, 'ml.m5.xlarge', accept = 'text/csv')\n",
    "\n",
    "# start a transform job\n",
    "sm_transformer.transform(test_data_path, split_type='Line', input_filter='$[1:]', content_type='text/csv')\n",
    "sm_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ba9fb-6628-4484-ac5e-0dd39f1668d2",
   "metadata": {},
   "source": [
    "##### The Batch Transform job will take about 4 minutes to complete after which you can evaluate the model results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f2efe7-0449-4b1c-8ee2-905dd866e2ce",
   "metadata": {},
   "source": [
    "### e. Copy and run the following code to evaluate the model metrics. Then, take a closer look at the code:\n",
    "\n",
    "##### First, you define a function that pulls the output of the Batch Transform job, which is contained in a file with a .out extension from the Amazon S3 bucket. Then, you extract the predicted labels into a dataframe and append the true labels to this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3f2ca-375f-40d3-be91-2719efdf74ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def get_csv_output_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:]\n",
    "    s3 = boto3.resource('s3')\n",
    "    obj = s3.Object(bucket_name, '{}/{}'.format(prefix, file_name))\n",
    "    return obj.get()[\"Body\"].read().decode('utf-8')\n",
    "output = get_csv_output_from_s3(sm_transformer.output_path, 'test_data.csv.out')\n",
    "output_df = pd.read_csv(io.StringIO(output), sep=\",\", header=None)\n",
    "output_df.head(8)\n",
    "output_df['Predicted']=np.round(output_df.values)\n",
    "output_df['Label'] = label\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_matrix = pd.crosstab(output_df['Predicted'], output_df['Label'], rownames=['Actual'], colnames=['Predicted'], margins = True)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25fe1f7-9573-4883-8010-a6f7e0fc83e1",
   "metadata": {},
   "source": [
    "##### You should see an output similar to the image, which shows the total number of Predicted True and False values compared to the Actual values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2294fe02-3fe2-463e-8099-728228132d1a",
   "metadata": {},
   "source": [
    "### f. Use the following code to extract both the baseline model accuracy and the model accuracy.\n",
    "\n",
    "#### Note: A helpful model for the baseline accuracy can be the fraction of non-default cases. A model that always predicts that a user will not default has that accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744bc516-1a42-4fc2-b967-d6cec919bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline Accuracy = {}\".format(1- np.unique(data['Label'], return_counts=True)[1][1]/(len(data['Label']))))\n",
    "print(\"Accuracy Score = {}\".format(accuracy_score(label, output_df['Predicted'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da5c35-8646-4fef-86a8-db63f0b75d44",
   "metadata": {},
   "source": [
    "#### The results show that a simple model can already beat the baseline accuracy. In order to improve the results, you can tune the hyperparameters. You can use hyperparameter optimization (HPO) on SageMaker for automatic model tuning. To learn more, see How Hyperparameter Tuning Works. \n",
    "\n",
    "#### Note: Although it is not included in this tutorial, you also have the option of including Batch Transform as part of your trial. When you call the .transform function, simply pass in the experiment_config as you did for the Training job. Amazon SageMaker automatically associates the Batch Transform as a trial component.\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-accuracy.06e79010a31766ea2511406de7618f2f1f3c1960.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a121f1dd-1cc2-4fed-ac19-59534f539bce",
   "metadata": {},
   "source": [
    "# Step 7: Deploy the model as an endpoint and set up data capture\n",
    "\n",
    "#### In this step, you deploy the model as a RESTful HTTPS endpoint to serve live inferences. Amazon SageMaker automatically handles the model hosting and creation of the endpoint for you.\n",
    "#### Complete the following steps to deploy the model as an endpoint and set up data capture.\n",
    "### a. Copy and paste the following code and choose Run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca8789-e820-4780-a04b-e7ea025e350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "latest_training_job = sm_client.list_training_jobs(MaxResults=1,\n",
    "                                                SortBy='CreationTime',\n",
    "                                                SortOrder='Descending')\n",
    "\n",
    "training_job_name=TrainingJobName=latest_training_job['TrainingJobSummaries'][0]['TrainingJobName']\n",
    "\n",
    "training_job_description = sm_client.describe_training_job(TrainingJobName=training_job_name)\n",
    "\n",
    "model_data = training_job_description['ModelArtifacts']['S3ModelArtifacts']\n",
    "container_uri = training_job_description['AlgorithmSpecification']['TrainingImage']\n",
    "\n",
    "# create a model.\n",
    "def create_model(role, model_name, container_uri, model_data):\n",
    "    return sm_client.create_model(\n",
    "        ModelName=model_name,\n",
    "        PrimaryContainer={\n",
    "        'Image': container_uri,\n",
    "        'ModelDataUrl': model_data,\n",
    "        },\n",
    "        ExecutionRoleArn=role)\n",
    "    \n",
    "\n",
    "try:\n",
    "    model = create_model(role, training_job_name, container_uri, model_data)\n",
    "except Exception as e:\n",
    "        sm_client.delete_model(ModelName=training_job_name)\n",
    "        model = create_model(role, training_job_name, container_uri, model_data)\n",
    "        \n",
    "\n",
    "print('Model created: '+model['ModelArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ddfe5a-f2b8-420e-ab65-5a1a1ee7fc73",
   "metadata": {},
   "source": [
    "### b. To specify the data configuration settings, copy and paste the following code and choose Run.\n",
    "\n",
    "#### This code tells SageMaker to capture 100% of the inference payloads received by the endpoint, capture both inputs and outputs, and also note the input content type as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23daa5a-3096-44cc-aa07-f985fb88ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_capture_upload_path = 's3://{}/{}/monitoring/datacapture'.format(rawbucket, prefix)\n",
    "data_capture_configuration = {\n",
    "    \"EnableCapture\": True,\n",
    "    \"InitialSamplingPercentage\": 100,\n",
    "    \"DestinationS3Uri\": s3_capture_upload_path,\n",
    "    \"CaptureOptions\": [\n",
    "        { \"CaptureMode\": \"Output\" },\n",
    "        { \"CaptureMode\": \"Input\" }\n",
    "    ],\n",
    "    \"CaptureContentTypeHeader\": {\n",
    "       \"CsvContentTypes\": [\"text/csv\"],\n",
    "       \"JsonContentTypes\": [\"application/json\"]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc8c383-e54b-4a97-8f45-318be0b4ea5e",
   "metadata": {},
   "source": [
    "### c. Copy and paste the following code and choose Run. This step creates an endpoint configuration and deploys the endpoint. In the code, you can specify instance type and whether you want to send all the traffic to this endpoint, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eebfd46-0034-4202-809b-864c7a4c1ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_endpoint_config(model_config, data_capture_config): \n",
    "    return sm_client.create_endpoint_config(\n",
    "                                                EndpointConfigName=model_config,\n",
    "                                                ProductionVariants=[\n",
    "                                                        {\n",
    "                                                            'VariantName': 'AllTraffic',\n",
    "                                                            'ModelName': model_config,\n",
    "                                                            'InitialInstanceCount': 1,\n",
    "                                                            'InstanceType': 'ml.m4.xlarge',\n",
    "                                                            'InitialVariantWeight': 1.0,\n",
    "                                                },\n",
    "                                                    \n",
    "                                                    ],\n",
    "                                                DataCaptureConfig=data_capture_config\n",
    "                                                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\n",
    "except Exception as e:\n",
    "    sm_client.delete_endpoint_config(EndpointConfigName=endpoint)\n",
    "    endpoint_config = create_endpoint_config(training_job_name, data_capture_configuration)\n",
    "\n",
    "print('Endpoint configuration created: '+ endpoint_config['EndpointConfigArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd9769-b9aa-4921-b07d-684eddbe5d73",
   "metadata": {},
   "source": [
    "### d. Copy and paste the following code and choose Run to create the endpoint.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc0129-4539-498d-bee8-a9a9a43d1956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable data capture, sampling 100% of the data for now. Next we deploy the endpoint in the correct VPC.\n",
    "\n",
    "endpoint_name = training_job_name\n",
    "def create_endpoint(endpoint_name, config_name):\n",
    "    return sm_client.create_endpoint(\n",
    "                                    EndpointName=endpoint_name,\n",
    "                                    EndpointConfigName=training_job_name\n",
    "                                )\n",
    "\n",
    "\n",
    "try:\n",
    "    endpoint = create_endpoint(endpoint_name, endpoint_config)\n",
    "except Exception as e:\n",
    "    sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint = create_endpoint(endpoint_name, endpoint_config)\n",
    "\n",
    "print('Endpoint created: '+ endpoint['EndpointArn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203d739b-a5a2-4214-b177-15fa914e9c13",
   "metadata": {},
   "source": [
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-endpoint-created.462d228b19d0aa86c03bb1a72d1af56a71bb34f5.png\" alt=\"Italian Trulli\">\n",
    "\n",
    "### e. In the left toolbar, choose Endpoints. The Endpoints list displays all of the endpoints in service.\n",
    "\n",
    "#### Notice the build-train-deploy endpoint shows a status of Creating. To deploy the model, Amazon SageMaker must first copy your model artifacts and inference image onto the instance and set up a HTTPS endpoint to inferface with client applications or RESTful APIs.\n",
    "\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-endpoint-status.52db936293c7da09a14e87bb0323ff66cd07c721.png\" alt=\"Italian Trulli\">\n",
    "\n",
    "\n",
    "#### Once the endpoint is created, the status changes to InService. (Note that creating an endpoint may take about 5-10 minutes.)  \n",
    "\n",
    "#### Note: You may need to click Refresh to get the updated status.\n",
    "\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-endpoints-inservice.e905d3fa36dd3a238515588c6fd57cfe1928e6d3.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce06c0d-4eb9-4344-ba44-f123c054b528",
   "metadata": {},
   "source": [
    "### f. In the JupyterLab Notebook, copy and run the following code to take a sample of the test dataset. This code takes the first 10 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9519ef-60e9-4d91-858a-57dada3fa7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -10 test_data.csv > test_sample.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7b1a3-d820-42a6-ad00-87cea2e5a383",
   "metadata": {},
   "source": [
    "### g. Run the following code to send some inference requests to this endpoint.\n",
    "\n",
    "#### Note: If you specified a different endpoint name, you will need to replace endpoint below with your endpoint name.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de523c5f-0672-4228-95ba-5e1d4d2f1cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import RealTimePredictor\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "predictor = RealTimePredictor(endpoint=endpoint_name, content_type = 'text/csv')\n",
    "\n",
    "with open('test_sample.csv', 'r') as f:\n",
    "    for row in f:\n",
    "        payload = row.rstrip('\\n')\n",
    "        response = predictor.predict(data=payload[2:])\n",
    "        sleep(0.5)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ded54-f051-45c4-bd5e-6047c4dbefb1",
   "metadata": {},
   "source": [
    "### h. Run the following code to verify that Model Monitor is correctly capturing the incoming data.\n",
    "\n",
    "#### In the code, the current_endpoint_capture_prefix captures the directory path where your ModelMonitor outputs are stored. Navigate to your Amazon S3 bucket, to see if the prediction requests are being captured. Note that this location should match the s3_capture_upload_path in the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36099ed-0a33-4e2b-8684-ad1887a8cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the captured json files.\n",
    "data_capture_prefix = '{}/monitoring'.format(prefix)\n",
    "s3_client = boto3.Session().client('s3')\n",
    "current_endpoint_capture_prefix = '{}/datacapture/{}/AllTraffic'.format(data_capture_prefix, endpoint_name)\n",
    "print(current_endpoint_capture_prefix)\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=current_endpoint_capture_prefix)\n",
    "capture_files = [capture_file.get(\"Key\") for capture_file in result.get('Contents')]\n",
    "print(\"Found Capture Files:\")\n",
    "print(\"\\n \".join(capture_files))\n",
    "\n",
    "\n",
    "capture_files[0]\n",
    "\n",
    "# This might fail initally, but try again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197c2e1-ca76-4679-98aa-619a3435f406",
   "metadata": {},
   "source": [
    "#### The captured output indicates that data capture is configured and saving the incoming requests.  \n",
    "\n",
    "#### Note: If you initially see a Null response, the data may not have been synchronously loaded onto the Amazon S3 path when you first initialized the data capture. Wait about a minute and try again.\n",
    "\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-extract-json.b8d96a4ad7f35a372350477cd0d44732d817dd35.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2883a9-450d-46ed-8a0b-ee16bd7af18e",
   "metadata": {},
   "source": [
    "### i. Run the following code to extract the content of one of the json files and view the captured outputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25928a-2891-4a59-86fd-777efa2fcffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View contents of the captured file.\n",
    "def get_obj_body(bucket, obj_key):\n",
    "    return s3_client.get_object(Bucket=rawbucket, Key=obj_key).get('Body').read().decode(\"utf-8\")\n",
    "\n",
    "capture_file = get_obj_body(rawbucket, capture_files[0])\n",
    "print(json.dumps(json.loads(capture_file.split('\\n')[5]), indent = 2, sort_keys =True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c822db-b2b1-4550-ad31-5877e488611e",
   "metadata": {},
   "source": [
    "#### The output indicates that data capture is capturing both the input payload and the output of the model.  \n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-data-capture.ce3385bec24cc4418bbd5619bfacbda0b9172362.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a17425-3ce7-4110-b779-ce1f6aadcf55",
   "metadata": {},
   "source": [
    "# Step 8: Monitor the endpoint with SageMaker Model Monitor\n",
    "\n",
    "#### In this step, you enable SageMaker Model Monitor to monitor the deployed endpoint for data drift. To do so, you compare the payload and outputs sent to the model against a baseline and determine whether there is any drift in the input data, or the label.  \n",
    "\n",
    "#### Complete the following steps to enable model monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02f17f-0644-4e42-b808-ac1d9ee99bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prefix = prefix + \"/\" + endpoint_name\n",
    "baseline_prefix = model_prefix + '/baselining'\n",
    "baseline_data_prefix = baseline_prefix + '/data'\n",
    "baseline_results_prefix = baseline_prefix + '/results'\n",
    "\n",
    "baseline_data_uri = 's3://{}/{}'.format(rawbucket,baseline_data_prefix)\n",
    "baseline_results_uri = 's3://{}/{}'.format(rawbucket, baseline_results_prefix)\n",
    "train_data_header_location = \"s3://\" + rawbucket + '/' + prefix + '/train_headers'\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))\n",
    "print(train_data_header_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5cb06c-c88f-4d08-af81-28f069e145a6",
   "metadata": {},
   "source": [
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-monitor-baseline.abcd340eba0425223dc460d618c529d5afc42ef7.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8f9e91-6aa0-452c-82b4-7ea72db34b63",
   "metadata": {},
   "source": [
    "### b. Run the following code to set up a baseline job for Model Monitor to capture the statistics of the training data. To do this, Model Monitor uses the deequ library built on top of Apache Spark for conducting unit tests on data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f5e17-9f85-44ee-94b4-94773f4135ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600)\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=os.path.join(train_data_header_location, 'train_data_with_headers.csv'),\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf0b39-705b-49ef-955b-00e89c0d8dc3",
   "metadata": {},
   "source": [
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-baseline-suggestion-job.94077d3556b0cec3439b0fae3cdd90e02ba6308d.png\" alt=\"Italian Trulli\">\n",
    "\n",
    "#### Model Monitor sets up a separate instance, copies over the training data, and generates some statistics. The service generates a lot of Apache Spark logs, which you can ignore. Once the job is completed, you will see a Spark job completed output.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df7466-c0d2-4318-a457-a55b3677bcf9",
   "metadata": {},
   "source": [
    "### c. Run the following code to look at the outputs generated by the baseline job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04da9b3-df6b-48da-8212-b9a47062768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=baseline_results_prefix)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Files:\")\n",
    "print(\"\\n \".join(report_files))\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d53a4b-da15-4368-97f0-e3eb4b21366e",
   "metadata": {},
   "source": [
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-found-files.140817729d769be046cdcce8e9d077fd90cbb094.png\" alt=\"Italian Trulli\">\n",
    "\n",
    "#### You will see two files: constraints.json and statistics.json. Next, dive deeper into their contents.\n",
    "\n",
    "#### The code above converts the json output in /statistics.json into a pandas dataframe. Note how the deequ library infers the data type of the column, the presence or absence of Null or missing values, and statistical parameters such as the mean, min, max, sum, standard deviation, and sketch parameters for an input data stream\n",
    "\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-pandas-dataframe.05626b92265719c2560ca0fea5cd8b764e82c033.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91fb4d-9dd3-4486-a7ce-098aa01063ae",
   "metadata": {},
   "source": [
    "### Likewise, the constraints.json file consists of a number of constraints the training dataset obeys such as non-negativity of values, and the data type of the feature field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40318fe-1edb-46ee-b377-23f44b7ebd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4461a09-9451-4b35-a2d5-ab0b42be7a91",
   "metadata": {},
   "source": [
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-constraints.f6ff33dcef9c1683e56876735748e2239936e443.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b757e58-ee44-4781-8cad-a4330f67e43d",
   "metadata": {},
   "source": [
    "### d. Run the following code to set up the frequency for endpoint monitoring.\n",
    "\n",
    "#### You can specify daily or hourly. This code specifies an hourly frequency, but you may want to change this for production applications as hourly frequency will generate a lot of data. Model Monitor will produce a report consisting of all the violations it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08cf84-142c-440b-8651-f7b935faa421",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports_prefix = '{}/reports'.format(prefix)\n",
    "s3_report_path = 's3://{}/{}'.format(rawbucket,reports_prefix)\n",
    "print(s3_report_path)\n",
    "\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "mon_schedule_name = 'Built-train-deploy-model-monitor-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3a7f64-ea44-4489-a162-d60cc1517aae",
   "metadata": {},
   "source": [
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-monitoring-schedule.18c63636ce89f1afef2d023f7661170c17de0694.png\" alt=\"Italian Trulli\">\n",
    " \n",
    " ### Note that this code enables Amazon CloudWatch Metrics, which instructs Model Monitor to send outputs to CloudWatch. You can use this approach to trigger alarms using CloudWatch Alarms to let engineers or admins know when data drift has been detected.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c089a5-67e5-4671-82a3-0cd1f4f76648",
   "metadata": {},
   "source": [
    "# Step 9: Test SageMaker Model Monitor performance\n",
    "\n",
    "#### In this step, you evaluate Model Monitor against some sample data. Instead of sending the test payload as is, you modify the distribution of several features in the test payload to test that Model Monitor can detect the change.\n",
    "\n",
    "#### Complete the following steps to test the Model Monitor performance.\n",
    "\n",
    "### a. Run the following code to import the test data and generate some modified sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f03e2e-f6db-4ec4-a15a-628f0b659c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = data.columns\n",
    "test_full = pd.read_csv('test_data.csv', names = ['Label'] +['PAY_AMT1','BILL_AMT1'] + list(COLS[1:])[:11] + list(COLS[1:])[12:17] + list(COLS[1:])[18:]\n",
    ")\n",
    "test_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a55550-d666-47eb-a202-48a14cafd2c8",
   "metadata": {},
   "source": [
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-test-model-monitor.6e99408ff7c5f40815eae5f7da1a76fc1e876085.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be92677-3825-419d-9975-43e8f00aa776",
   "metadata": {},
   "source": [
    "### b. Run the following code to change a few columns. Note the differences marked in red in the image here from the previous step. Drop the label column and save the modified sample test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d231587-a5c5-45d2-a937-c9c037e146fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "faketestdata = test_full\n",
    "faketestdata['EDUCATION'] = -faketestdata['EDUCATION'].astype(float)\n",
    "faketestdata['BILL_AMT2']= (faketestdata['BILL_AMT2']//10).astype(float)\n",
    "faketestdata['AGE']= (faketestdata['AGE']-10).astype(float)\n",
    "\n",
    "#faketestdata.head()\n",
    "faketestdata.drop(columns=['Label']).to_csv('test-data-input-cols.csv', index = None, header=None)\n",
    "\n",
    "faketestdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33568e5b-fbc2-4293-9f05-4d104502885d",
   "metadata": {},
   "source": [
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-changed-data.9f7dd607cda6a7ab60a657e1b7aec16370ace339.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2291226-3f34-4fe6-aff9-11c985c2d974",
   "metadata": {},
   "source": [
    "# c. Run the following code to repeatedly invoke the endpoint with this modified dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea36ad9-8b03-43fe-8c7b-9119f65db485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "# (just repeating code from above for convenience/ able to run this section independently)\n",
    "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
    "    with open(file_name, 'r') as f:\n",
    "        for row in f:\n",
    "            payload = row.rstrip('\\n')\n",
    "            response = runtime_client.invoke_endpoint(EndpointName=endpoint_name,\n",
    "                                          ContentType='text/csv', \n",
    "                                          Body=payload)\n",
    "            time.sleep(1)\n",
    "            \n",
    "def invoke_endpoint_forever():\n",
    "    while True:\n",
    "        invoke_endpoint(endpoint, 'test-data-input-cols.csv', runtime_client)\n",
    "        \n",
    "thread = Thread(target = invoke_endpoint_forever)\n",
    "thread.start()\n",
    "# Note that you need to stop the kernel to stop the invocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec3979d-12b8-402a-a7fa-06b9d9ff9903",
   "metadata": {},
   "source": [
    "### d. Run the following code to check the status of the Model Monitor job.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6d93d4-9aa7-45c9-92c9-1989b030f82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce69b4-6b78-4d7a-97f7-213c86eb7318",
   "metadata": {},
   "source": [
    "### You should see an output of Schedule status: Scheduled\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c040d38a-3203-4c4c-882e-70a66448c659",
   "metadata": {},
   "source": [
    "### e. Run the following code to check every 10 minutes if any monitoring outputs have been generated. Note that the first job may run with a buffer of about 20 minutes.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac61d1-8a7c-4f78-ae0b-c07ddc77c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = my_default_monitor.list_executions()\n",
    "print(\"We created ahourly schedule above and it will kick off executions ON the hour (plus 0 - 20 min buffer.\\nWe will have to wait till we hit the hour...\")\n",
    "\n",
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(600)\n",
    "    mon_executions = my_default_monitor.list_executions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c93f2-07d0-496f-9ea3-801b041581f9",
   "metadata": {},
   "source": [
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-check-outputs.ff1fd4f19f425b4c550173751822b43d1d88c224.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230d5bf-17ed-4de9-a7b4-ef742fe4858d",
   "metadata": {},
   "source": [
    "### f. In the left toolbar of Amazon SageMaker Studio, choose Endpoints. Right-click the build-train-deploy endpoint and choose Describe Endpoint.\n",
    "\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-describe-endpoint.9fce4a1fc1edf1d313f047fcf3d7a08c3436342a.png\" alt=\"Italian Trulli\">\n",
    "\n",
    "### g. Choose Monitoring job history. Notice that the Monitoring status shows In progress.\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-job-in-progress.71f53f2b4495a140a46185926cec91d755081e84.png\" alt=\"Italian Trulli\">\n",
    "#### Once the job is complete, the Monitoring status displays Issue found (for any issues found).  \n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-job-complete.b737bd9c81ebba5e7965cfdcb503383e1f39eff2.png\" alt=\"Italian Trulli\">\n",
    "\n",
    "### h. Double-click the issue to view details. You can see that Model Monitor detected large baseline drifts in the EDUCATION and BILL_AMT2 fields that you previously modified.\n",
    "\n",
    "### Model Monitor also detected some differences in data types in two other fields. The training data consists of integer labels, but the XGBoost model predicts a probability score. Therefore, Model Monitor reported a mismatch.  \n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-monitoring-details.b7ec04489c35ab8e250881100cb8783d084fe1b4.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89537d66-a8c0-4087-872e-726f442fbd03",
   "metadata": {},
   "source": [
    "### i. In your JupyterLab Notebook, run the following cells to see the output from Model Monitor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f10bd0-5b8f-4ad4-a040-ff9eef489e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_execution = mon_executions[-1] # latest execution's index is -1, second to last is -2 and so on..\n",
    "time.sleep(60)\n",
    "latest_execution.wait(logs=False)\n",
    "\n",
    "print(\"Latest execution status: {}\".format(latest_execution.describe()['ProcessingJobStatus']))\n",
    "print(\"Latest execution result: {}\".format(latest_execution.describe()['ExitMessage']))\n",
    "\n",
    "latest_job = latest_execution.describe()\n",
    "if (latest_job['ProcessingJobStatus'] != 'Completed'):\n",
    "        print(\"====STOP==== \\n No completed executions to inspect further. Please wait till an execution completes or investigate previously reported failures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8eca0-eb2d-40d2-b570-b38a7101c1fc",
   "metadata": {},
   "source": [
    " <img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-job-complete-in-notebook.02e6e572fc344c036e6eb17bf5f89544474d04e6.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ad7310-b5a8-4db5-9a11-0d7b522f1b99",
   "metadata": {},
   "source": [
    "### j. Run the following code to view the reports generated by Model Monitor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ec070d-e734-4579-9f9b-db31f8d5cdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_uri=latest_execution.output.destination\n",
    "print('Report Uri: {}'.format(report_uri))\n",
    "from urllib.parse import urlparse\n",
    "s3uri = urlparse(report_uri)\n",
    "report_bucket = s3uri.netloc\n",
    "report_key = s3uri.path.lstrip('/')\n",
    "print('Report bucket: {}'.format(report_bucket))\n",
    "print('Report key: {}'.format(report_key))\n",
    "\n",
    "s3_client = boto3.Session().client('s3')\n",
    "result = s3_client.list_objects(Bucket=rawbucket, Prefix=report_key)\n",
    "report_files = [report_file.get(\"Key\") for report_file in result.get('Contents')]\n",
    "print(\"Found Report Files:\")\n",
    "print(\"\\n \".join(report_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f2248a-8625-4491-8b2a-32285afbad3f",
   "metadata": {},
   "source": [
    "#### You can see that in addition to statistics.json and constraints.json, there is a new file generated named constraint_violations.json. The contents of this file were displayed above in Amazon SageMaker Studio (Step g).\n",
    "\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-constraints-violation.bb9c8bfcfc663443b50f70ba6445c25bdd728c59.png\" alt=\"Italian Trulli\">\n",
    "\n",
    "#### Note: Once you set up data capture, Amazon SageMaker Studio automatically creates a notebook for you that contains the code above to run monitoring jobs. To access the notebook, right-click the endpoint and choose Describe Endpoint. On the Monitoring results tab, choose Enable Monitoring. This step automatically opens a Jupyter notebook containing the code you authored above.\n",
    "\n",
    "<img src=\"https://d1.awsstatic.com/Getting%20Started/tutorials/tutorial-sagemaker-studio-model-monitor.3d2d7aa2dbe60180cffe0fb4ba2dedb6a79c5273.png\" alt=\"Italian Trulli\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf55887-5a4e-4b92-8bff-580d7f28f4a5",
   "metadata": {},
   "source": [
    "# Step 10. Clean up\n",
    "\n",
    "#### In this step, you terminate the resources you used in this lab.\n",
    "\n",
    "#### Important: Terminating resources that are not actively being used reduces costs and is a best practice. Not terminating your resources will result in charges to your account.\n",
    "\n",
    "### a. Delete monitoring schedules: In your Jupyter notebook, copy and paste the following code and choose Run.\n",
    "\n",
    "#### Note: You cannot delete the Model Monitor endpoint until all of the monitoring jobs associated with the endpoint are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad0e044-f465-45f7-9206-ac746b31a20d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_default_monitor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0aeeb3f15375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_default_monitor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_monitoring_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# actually wait for the deletion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_default_monitor' is not defined"
     ]
    }
   ],
   "source": [
    "my_default_monitor.delete_monitoring_schedule()\n",
    "time.sleep(10) # actually wait for the deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90be9a5-917d-4f24-829a-7d0b1928f108",
   "metadata": {},
   "source": [
    "### b. Delete your endpoint: In your Jupyter notebook, copy and paste the following code and choose Run.\n",
    "\n",
    "#### Note: Make sure you have first deleted all monitoring jobs associated with the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76770b13-0ebf-485c-b4d0-8bf955f6268e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1b33e393a503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sm' is not defined"
     ]
    }
   ],
   "source": [
    "sm.delete_endpoint(EndpointName = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74914dc-a9ee-43ac-aa7b-7d052c47154c",
   "metadata": {},
   "source": [
    "### If you want to clean up all training artifacts (models, preprocessed data sets, etc.), copy and paste the following code into your code cell and choose Run.\n",
    "\n",
    "#### Note: Make sure to replace ACCOUNT_NUMBER with your account number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982532f9-849e-4b49-bbab-d9d9e7dfab1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://sagemaker-us-east-1-902607334202/sagemaker-modelmonitor/data/rawdata.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "aws s3 rm --recursive s3://sagemaker-us-east-1-902607334202/sagemaker-modelmonitor/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51c54f1-82b2-4409-a0d6-5a7f69a20b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
